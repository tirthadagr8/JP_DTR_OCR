{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10582380,"sourceType":"datasetVersion","datasetId":6548844}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U neptune\n%pip install evaluate\n%pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:33:29.834592Z","iopub.execute_input":"2025-02-01T17:33:29.834862Z","iopub.status.idle":"2025-02-01T17:33:46.292978Z","shell.execute_reply.started":"2025-02-01T17:33:29.834840Z","shell.execute_reply":"2025-02-01T17:33:46.291987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:33:46.294269Z","iopub.execute_input":"2025-02-01T17:33:46.294577Z","iopub.status.idle":"2025-02-01T17:34:06.472391Z","shell.execute_reply.started":"2025-02-01T17:33:46.294552Z","shell.execute_reply":"2025-02-01T17:34:06.471203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport xml.etree.ElementTree as ET\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nimport os\nimport pandas as pd\nimport numpy as np\nimport math\nfrom tqdm import tqdm\nimport time\nfrom datetime import datetime\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom torchvision import transforms as T\nimport torchvision\nfrom transformers import VisionEncoderDecoderModel, AutoFeatureExtractor, AutoTokenizer\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom datasets import load_dataset\nimport torch\n\ndevice=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:23:14.253899Z","iopub.execute_input":"2025-02-01T20:23:14.254177Z","iopub.status.idle":"2025-02-01T20:23:36.790150Z","shell.execute_reply.started":"2025-02-01T20:23:14.254154Z","shell.execute_reply":"2025-02-01T20:23:36.789426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config={\n    'encoder_model_name':'google/vit-base-patch16-224-in21k', # as encoder\n    'decoder_model_name':'dis|tilgpt2',     # DistilGPT-2 as the decoder\n    'feature_extractor_name':'microsoft/resnet-50',\n    'tokenizer_name':'tirthadagr8/CustomOCR',\n    \n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:23:36.791115Z","iopub.execute_input":"2025-02-01T20:23:36.791735Z","iopub.status.idle":"2025-02-01T20:23:36.795097Z","shell.execute_reply.started":"2025-02-01T20:23:36.791700Z","shell.execute_reply":"2025-02-01T20:23:36.794269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images=[]\nannotations = []\nlabels=[]\nraw_path='/kaggle/input/manga109-synthetic/data/manga/Manga109/'\nsynth_path='/kaggle/input/manga109-synthetic/data/manga/synthetic/'\nraw_manga=pd.read_csv(raw_path+'data.csv')\nsynthetic_manga=pd.read_csv(synth_path+'meta/0000.csv')\ndrop_rows=[]\nfor i,text in enumerate(raw_manga['text']):\n    if len(text)>=62:\n        drop_rows.append(i)\nraw_manga.drop(drop_rows,inplace=True)\nprint(f'Removed {len(drop_rows)} rows from raw_manga')\ndrop_rows=[]\nfor i,text in enumerate(synthetic_manga['text']):\n    if len(text)>=62:\n        drop_rows.append(i)\nsynthetic_manga.drop(drop_rows,inplace=True)\nprint(f'Removed {len(drop_rows)} rows from synthetic_manga')\nraw_manga['path']=raw_path+raw_manga['crop_path']\nsynthetic_manga['path']=synth_path+'img/0000/'+synthetic_manga['id']+'.jpg'\nimages=list(raw_manga['path'].values)+list(synthetic_manga['path'].values)\nlabels=list(raw_manga['text'].values)+list(synthetic_manga['text'].values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:31:14.133516Z","iopub.execute_input":"2025-02-01T20:31:14.133825Z","iopub.status.idle":"2025-02-01T20:31:16.080536Z","shell.execute_reply.started":"2025-02-01T20:31:14.133803Z","shell.execute_reply":"2025-02-01T20:31:16.079582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_chars=['â–‘']+sorted(list(set(''.join(labels))))\nchar_to_int={j:i for i,j in enumerate(all_chars)}\nint_to_char={i:j for i,j in enumerate(all_chars)}\n\ndef text_operation(s=None,arr=None,padding=False,return_tensor=True,max_length=64,remove_padding=False):\n    if s is not None:\n        ans=[]\n        for i in s:\n            ans.append(char_to_int[i])\n        if padding:\n            ans+=[0]*(max_length-len(ans))\n        if return_tensor:\n            return torch.tensor(ans,dtype=torch.long)\n    if arr is not None:\n        ans=''\n        if isinstance(arr,torch.Tensor):\n            arr=arr.clone().detach().cpu().numpy()\n        for i in arr:\n            if not (remove_padding and i==0):\n                ans+=int_to_char[i]\n        return ans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:31:22.099181Z","iopub.execute_input":"2025-02-01T20:31:22.099542Z","iopub.status.idle":"2025-02-01T20:31:22.545137Z","shell.execute_reply.started":"2025-02-01T20:31:22.099512Z","shell.execute_reply":"2025-02-01T20:31:22.544178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport torch\nfrom transformers import AutoTokenizer\n\nclass DatasetLoaderPrivate2(Dataset):\n    def __init__(self, images, labels) -> None:\n        super().__init__()\n        self.transforms = T.Compose([\n            T.Resize((224, 224)),\n            T.RandomPerspective(distortion_scale=0.2, p=0.5),\n            T.RandomRotation(5),\n            T.GaussianBlur(3),\n            T.RandomAdjustSharpness(2),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        self.simple_transforms = T.Compose([\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        self.tokenizer = AutoTokenizer.from_pretrained('tirthadagr8/CustomOCR')\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        augment = False\n        if index >= len(self.images):\n            index %= len(self.images)\n            augment = True\n        \n        # Apply image transformations\n        if augment:\n            img = self.transforms(Image.open(self.images[index]).convert('RGB'))\n        else:\n            img = self.simple_transforms(Image.open(self.images[index]).convert('RGB'))\n\n        # Tokenize the label text\n        encoded_text = self.tokenizer.encode_plus(\n            f'{self.tokenizer.bos_token}{self.labels[index]}{self.tokenizer.eos_token}',\n            padding='max_length',\n            max_length=64,\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        # Extract the input_ids and attention_mask\n        input_ids = encoded_text['input_ids'].squeeze(0)  # Remove batch dimension\n        attention_mask = encoded_text['attention_mask'].squeeze(0)  # Remove batch dimension\n\n        return {\n            'pixel_values': img,\n            'input_ids': input_ids,\n            'labels': input_ids,  # Typically, labels are the same as input_ids in language modeling tasks\n            'attention_mask': attention_mask\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:35:26.705900Z","iopub.execute_input":"2025-02-01T20:35:26.706215Z","iopub.status.idle":"2025-02-01T20:35:26.740169Z","shell.execute_reply.started":"2025-02-01T20:35:26.706191Z","shell.execute_reply":"2025-02-01T20:35:26.739151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__=='__main__':\n    dd=DatasetLoaderPrivate2(images,labels)\n    d=DataLoader(dd,batch_size=1,shuffle=True)\n    tokenizer=AutoTokenizer.from_pretrained('tirthadagr8/CustomOCR')\n    import matplotlib.pyplot as plt\n    c=0\n    ttt=[]\n    for t in tqdm(d):\n#         for l in t[1][0]:\n#             ttt.append(torch.argmax(l).item())\n        print(tokenizer.decode(t['labels'][0],skip_special_tokens=True))\n#         print(dd.tokenizer.decode(t[1][0],skip_special_token=True))\n        # if c>8:\n        #     break\n        c+=1\n        plt.imshow(t['pixel_values'][0].detach().cpu().permute(1,2,0))\n        plt.show()\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T20:35:40.622451Z","iopub.execute_input":"2025-02-01T20:35:40.622859Z","iopub.status.idle":"2025-02-01T20:35:41.197574Z","shell.execute_reply.started":"2025-02-01T20:35:40.622828Z","shell.execute_reply":"2025-02-01T20:35:41.196584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Create VisionEncoderDecoderModel\ntokenizer=AutoTokenizer.from_pretrained(config['tokenizer_name'])\nfrom transformers import ViTConfig, GPT2Config, VisionEncoderDecoderConfig\n\n# Custom encoder config\nencoder_config = ViTConfig(\n    image_size=224,\n    patch_size=16,\n    num_channels=3,\n    hidden_size=768,  # Original: 384 (vit-small)\n    num_hidden_layers=12,  # Reduced from 12\n    num_attention_heads=12,  # Original: 6\n    intermediate_size=3072,\n    hidden_act=\"gelu\",\n    hidden_dropout_prob=0.1,\n    attention_probs_dropout_prob=0.1,\n    initializer_range=0.02,\n    layer_norm_eps=1e-6,\n    is_training=True,\n    patch_attention= True  # Custom: Enable patch-wise attention\n)\n\n# Custom decoder config\ndecoder_config = GPT2Config(\n    vocab_size=len(tokenizer.vocab),  # Original: 50257\n    n_positions=512,  # Reduced from 1024 for OCR\n    n_embd=768,\n    n_layer=12,  # Original: 12\n    n_head=12,  # Original: 12\n    n_inner=2048,\n    activation_function=\"gelu_new\",\n    resid_pdrop=0.1,\n    embd_pdrop=0.1,\n    attn_pdrop=0.1,\n    layer_norm_epsilon=1e-5,\n    initializer_range=0.02,\n    scale_attn_weights=True,\n    use_cache=False,  # Disable for training\n    bos_token_id=tokenizer.bos_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.pad_token_id,\n    add_cross_attention=True  # Essential for encoder-decoder\n)\nmain_config=VisionEncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n# model=VisionEncoderDecoderModel(main_config)\nfrom transformers import VisionEncoderDecoderModel\n\nmodel=VisionEncoderDecoderModel.from_pretrained('tirthadagr8/CustomOCR')\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Trainable parameters: {count_parameters(model)/1e6:.1f}M\")\n# Step 3: Set Special Tokens and Configuration\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.eos_token_id = tokenizer.eos_token_id\n\n# Optional: Adjust max length for generation\nmodel.config.max_length = 64  # Maximum output sequence length\n\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams=5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:58:01.457367Z","iopub.execute_input":"2025-02-01T16:58:01.457656Z","iopub.status.idle":"2025-02-01T16:58:24.764470Z","shell.execute_reply.started":"2025-02-01T16:58:01.457634Z","shell.execute_reply":"2025-02-01T16:58:24.763800Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import neptune\nfrom kaggle_secrets import UserSecretsClient\n\nneptune_api = UserSecretsClient().get_secret(\"NEPTUNE_API_TOKEN\")\nneptune_project =UserSecretsClient().get_secret(\"NEPTUNE_PROJECT\")\n\nfrom transformers.integrations import NeptuneCallback\nneptune_callback = NeptuneCallback(\n    project=neptune_project,\n    api_token=neptune_api\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:59:05.705800Z","iopub.execute_input":"2025-02-01T16:59:05.706160Z","iopub.status.idle":"2025-02-01T16:59:06.869599Z","shell.execute_reply.started":"2025-02-01T16:59:05.706132Z","shell.execute_reply":"2025-02-01T16:59:06.868936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nclass Metrics:\n    def __init__(self, tokenizer):\n        self.cer_metric = evaluate.load(\"cer\")\n        self.processor = tokenizer\n\n    def compute_metrics(self, pred):\n        label_ids = pred.label_ids\n        pred_ids = pred.predictions\n        print(label_ids.shape, pred_ids.shape)\n\n        pred_str = self.processor.batch_decode(pred_ids, skip_special_tokens=True)\n        label_ids[label_ids == -100] = self.processor.pad_token_id\n        label_str = self.processor.batch_decode(label_ids, skip_special_tokens=True)\n\n        pred_str = np.array([\"\".join(text.split()) for text in pred_str])\n        label_str = np.array([\"\".join(text.split()) for text in label_str])\n\n        results = {}\n        try:\n            results[\"cer\"] = self.cer_metric.compute(predictions=pred_str, references=label_str)\n        except Exception as e:\n            print(e)\n            print(pred_str)\n            print(label_str)\n            results[\"cer\"] = 0\n        results[\"accuracy\"] = (pred_str == label_str).mean()\n\n        return results\nmetrics=Metrics(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:59:09.856249Z","iopub.execute_input":"2025-02-01T16:59:09.856538Z","iopub.status.idle":"2025-02-01T16:59:10.534091Z","shell.execute_reply.started":"2025-02-01T16:59:09.856516Z","shell.execute_reply":"2025-02-01T16:59:10.533311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size=32\ntrain_dataset = DatasetLoaderPrivate2(images[:-1000], labels[:-1000])\nval_dataset = DatasetLoaderPrivate2(images[-1000:], labels[-1000:])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n\n# Step 5: Define Training Arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./ocr-model\",\n    learning_rate=5e-5,\n    predict_with_generate=True,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    lr_scheduler_type='cosine',\n    num_train_epochs=2,\n    logging_dir=\"./logs\",\n    logging_steps=2,\n    max_steps=30000,\n    save_steps=1000,\n    eval_steps=1000,\n    save_total_limit=1,\n    report_to='none',\n    fp16=True,  # Enable mixed precision training if GPU supports it\n)\n\n# Step 6: Define Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=metrics.compute_metrics,\n    data_collator=None,  # Default collator works for VisionEncoderDecoderModel\n    callbacks=[neptune_callback]\n)\n\n# Step 7: Train the Model\ntrainer.train()\n\n# Step 8: Save the Model\nmodel.save_pretrained(\"./ocr-model\")\ntokenizer.save_pretrained(\"./ocr-model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:00:42.195177Z","iopub.execute_input":"2025-02-01T17:00:42.195465Z","iopub.status.idle":"2025-02-01T17:01:44.003103Z","shell.execute_reply.started":"2025-02-01T17:00:42.195445Z","shell.execute_reply":"2025-02-01T17:01:44.001862Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.batch_decode(model.cuda().generate(t['pixel_values'][0].unsqueeze(0).cuda()),skip_special_tokens=True)[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:03:27.939993Z","iopub.execute_input":"2025-02-01T17:03:27.940301Z","iopub.status.idle":"2025-02-01T17:03:28.369988Z","shell.execute_reply.started":"2025-02-01T17:03:27.940280Z","shell.execute_reply":"2025-02-01T17:03:28.369233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(t['pixel_values'][0].detach().cpu().permute(1,2,0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T17:02:10.536548Z","iopub.execute_input":"2025-02-01T17:02:10.536844Z","iopub.status.idle":"2025-02-01T17:02:10.730087Z","shell.execute_reply.started":"2025-02-01T17:02:10.536822Z","shell.execute_reply":"2025-02-01T17:02:10.729369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}